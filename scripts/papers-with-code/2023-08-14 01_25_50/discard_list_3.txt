14
What does 2D geometric information really tell us about 3D face shape?
2017-08-22
A face image contains geometric cues in the form of configurational information and contours that can be used to estimate 3D face shape. While it is clear that 3D reconstruction from 2D points is highly ambiguous if no further constraints are enforced, one might expect that the face-space constraint solves this problem. We show that this is not the case and that geometric information is an ambiguous cue. There are two sources for this ambiguity. The first is that, within the space of 3D face shapes, there are flexibility modes that remain when some parts of the face are fixed. The second occurs only under perspective projection and is a result of perspective transformation as camera distance varies. Two different faces, when viewed at different distances, can give rise to the same 2D geometry. To demonstrate these ambiguities, we develop new algorithms for fitting a 3D morphable model to 2D landmarks or contours under either orthographic or perspective projection and show how to compute flexibility modes for both cases. We show that both fitting problems can be posed as a separable nonlinear least squares problem and solved efficiently. We demonstrate both quantitatively and qualitatively that the ambiguity is present in reconstructions from geometric information alone but also in reconstructions from a state-of-the-art CNN-based method.

28
3D Reconstruction of Incomplete Archaeological Objects Using a Generative Adversarial Network
2017-11-17
We introduce a data-driven approach to aid the repairing and conservation of
archaeological objects: ORGAN, an object reconstruction generative adversarial
network (GAN). By using an encoder-decoder 3D deep neural network on a GAN
architecture, and combining two loss objectives: a completion loss and an
Improved Wasserstein GAN loss, we can train a network to effectively predict
the missing geometry of damaged objects. As archaeological objects can greatly
differ between them, the network is conditioned on a variable, which can be a
culture, a region or any metadata of the object. In our results, we show that
our method can recover most of the information from damaged objects, even in
cases where more than half of the voxels are missing, without producing many
errors.

50
Image2Mesh: A Learning Framework for Single Image 3D Reconstruction
2017-11-29
One challenge that remains open in 3D deep learning is how to efficiently
represent 3D data to feed deep networks. Recent works have relied on volumetric
or point cloud representations, but such approaches suffer from a number of
issues such as computational complexity, unordered data, and lack of finer
geometry. This paper demonstrates that a mesh representation (i.e. vertices and
faces to form polygonal surfaces) is able to capture fine-grained geometry for
3D reconstruction tasks. A mesh however is also unstructured data similar to
point clouds. We address this problem by proposing a learning framework to
infer the parameters of a compact mesh representation rather than learning from
the mesh itself. This compact representation encodes a mesh using free-form
deformation and a sparse linear combination of models allowing us to
reconstruct 3D meshes from single images. In contrast to prior work, we do not
rely on silhouettes and landmarks to perform 3D reconstruction. We evaluate our
method on synthetic and real-world datasets with very promising results. Our
framework efficiently reconstructs 3D objects in a low-dimensional way while
preserving its important geometrical aspects.

51
Highlighting objects of interest in an image by integrating saliency and depth
2017-11-28
Stereo images have been captured primarily for 3D reconstruction in the past.
However, the depth information acquired from stereo can also be used along with
saliency to highlight certain objects in a scene. This approach can be used to
make still images more interesting to look at, and highlight objects of
interest in the scene. We introduce this novel direction in this paper, and
discuss the theoretical framework behind the approach. Even though we use depth
from stereo in this work, our approach is applicable to depth data acquired
from any sensor modality. Experimental results on both indoor and outdoor
scenes demonstrate the benefits of our algorithm.

55
Learning a Hierarchical Latent-Variable Model of 3D Shapes
2017-05-17
We propose the Variational Shape Learner (VSL), a generative model that
learns the underlying structure of voxelized 3D shapes in an unsupervised
fashion. Through the use of skip-connections, our model can successfully learn
and infer a latent, hierarchical representation of objects. Furthermore,
realistic 3D objects can be easily generated by sampling the VSL's latent
probabilistic manifold. We show that our generative model can be trained
end-to-end from 2D images to perform single image 3D model retrieval.
Experiments show, both quantitatively and qualitatively, the improved
generalization of our proposed model over a range of tasks, performing better
or comparable to various state-of-the-art alternatives.

58
OctNetFusion: Learning Depth Fusion from Data
2017-04-04
In this paper, we present a learning based approach to depth fusion, i.e.,
dense 3D reconstruction from multiple depth images. The most common approach to
depth fusion is based on averaging truncated signed distance functions, which
was originally proposed by Curless and Levoy in 1996. While this method is
simple and provides great results, it is not able to reconstruct (partially)
occluded surfaces and requires a large number frames to filter out sensor noise
and outliers. Motivated by the availability of large 3D model repositories and
recent advances in deep learning, we present a novel 3D CNN architecture that
learns to predict an implicit surface representation from the input depth maps.
Our learning based method significantly outperforms the traditional volumetric
fusion approach in terms of noise reduction and outlier suppression. By
learning the structure of real world 3D objects and scenes, our approach is
further able to reconstruct occluded regions and to fill in gaps in the
reconstruction. We demonstrate that our learning based approach outperforms
both vanilla TSDF fusion as well as TV-L1 fusion on the task of volumetric
fusion. Further, we demonstrate state-of-the-art 3D shape completion results.

59
Intel RealSense Stereoscopic Depth Cameras
2017-05-16
We present a comprehensive overview of the stereoscopic Intel RealSense RGBD
imaging systems. We discuss these systems' mode-of-operation, functional
behavior and include models of their expected performance, shortcomings, and
limitations. We provide information about the systems' optical characteristics,
their correlation algorithms, and how these properties can affect different
applications, including 3D reconstruction and gesture recognition. Our
discussion covers the Intel RealSense R200 and the Intel RealSense D400
(formally RS400).

60
Large-Scale 3D Shape Reconstruction and Segmentation from ShapeNet Core55
2017-10-17
We introduce a large-scale 3D shape understanding benchmark using data and
annotation from ShapeNet 3D object database. The benchmark consists of two
tasks: part-level segmentation of 3D shapes and 3D reconstruction from single
view images. Ten teams have participated in the challenge and the best
performing teams have outperformed state-of-the-art approaches on both tasks. A
few novel deep learning architectures have been proposed on various 3D
representations on both tasks. We report the techniques used by each team and
the corresponding performances. In addition, we summarize the major discoveries
from the reported results and possible trends for the future work in the field.

61
Real-Time Automatic Fetal Brain Extraction in Fetal MRI by Deep Learning
2017-10-25
Brain segmentation is a fundamental first step in neuroimage analysis. In the
case of fetal MRI, it is particularly challenging and important due to the
arbitrary orientation of the fetus, organs that surround the fetal head, and
intermittent fetal motion. Several promising methods have been proposed but are
limited in their performance in challenging cases and in real-time
segmentation. We aimed to develop a fully automatic segmentation method that
independently segments sections of the fetal brain in 2D fetal MRI slices in
real-time. To this end, we developed and evaluated a deep fully convolutional
neural network based on 2D U-net and autocontext, and compared it to two
alternative fast methods based on 1) a voxelwise fully convolutional network
and 2) a method based on SIFT features, random forest and conditional random
field. We trained the networks with manual brain masks on 250 stacks of
training images, and tested on 17 stacks of normal fetal brain images as well
as 18 stacks of extremely challenging cases based on extreme motion, noise, and
severely abnormal brain shape. Experimental results show that our U-net
approach outperformed the other methods and achieved average Dice metrics of
96.52% and 78.83% in the normal and challenging test sets, respectively. With
an unprecedented performance and a test run time of about 1 second, our network
can be used to segment the fetal brain in real-time while fetal MRI slices are
being acquired. This can enable real-time motion tracking, motion detection,
and 3D reconstruction of fetal brain MRI.

65
Weakly supervised 3D Reconstruction with Adversarial Constraint
2017-05-31
Supervised 3D reconstruction has witnessed a significant progress through the
use of deep neural networks. However, this increase in performance requires
large scale annotations of 2D/3D data. In this paper, we explore inexpensive 2D
supervision as an alternative for expensive 3D CAD annotation. Specifically, we
use foreground masks as weak supervision through a raytrace pooling layer that
enables perspective projection and backpropagation. Additionally, since the 3D
reconstruction from masks is an ill posed problem, we propose to constrain the
3D reconstruction to the manifold of unlabeled realistic 3D shapes that match
mask observations. We demonstrate that learning a log-barrier solution to this
constrained optimization problem resembles the GAN objective, enabling the use
of existing tools for training GANs. We evaluate and analyze the manifold
constrained reconstruction on various datasets for single and multi-view
reconstruction of both synthetic and real images.

66
3D Shape Reconstruction from Sketches via Multi-view Convolutional Networks
2017-07-20
We propose a method for reconstructing 3D shapes from 2D sketches in the form
of line drawings. Our method takes as input a single sketch, or multiple
sketches, and outputs a dense point cloud representing a 3D reconstruction of
the input sketch(es). The point cloud is then converted into a polygon mesh. At
the heart of our method lies a deep, encoder-decoder network. The encoder
converts the sketch into a compact representation encoding shape information.
The decoder converts this representation into depth and normal maps capturing
the underlying surface from several output viewpoints. The multi-view maps are
then consolidated into a 3D point cloud by solving an optimization problem that
fuses depth and normals across all viewpoints. Based on our experiments,
compared to other methods, such as volumetric networks, our architecture offers
several advantages, including more faithful reconstruction, higher output
surface resolution, better preservation of topology and shape structure.

67
LabelFusion: A Pipeline for Generating Ground Truth Labels for Real RGBD Data of Cluttered Scenes
2017-07-15
Deep neural network (DNN) architectures have been shown to outperform
traditional pipelines for object segmentation and pose estimation using RGBD
data, but the performance of these DNN pipelines is directly tied to how
representative the training data is of the true data. Hence a key requirement
for employing these methods in practice is to have a large set of labeled data
for your specific robotic manipulation task, a requirement that is not
generally satisfied by existing datasets. In this paper we develop a pipeline
to rapidly generate high quality RGBD data with pixelwise labels and object
poses. We use an RGBD camera to collect video of a scene from multiple
viewpoints and leverage existing reconstruction techniques to produce a 3D
dense reconstruction. We label the 3D reconstruction using a human assisted
ICP-fitting of object meshes. By reprojecting the results of labeling the 3D
scene we can produce labels for each RGBD image of the scene. This pipeline
enabled us to collect over 1,000,000 labeled object instances in just a few
days. We use this dataset to answer questions related to how much training data
is required, and of what quality the data must be, to achieve high performance
from a DNN architecture.

80
Learning a Multi-View Stereo Machine
2017-08-17
We present a learnt system for multi-view stereopsis. In contrast to recent
learning based methods for 3D reconstruction, we leverage the underlying 3D
geometry of the problem through feature projection and unprojection along
viewing rays. By formulating these operations in a differentiable manner, we
are able to learn the system end-to-end for the task of metric 3D
reconstruction. End-to-end learning allows us to jointly reason about shape
priors while conforming geometric constraints, enabling reconstruction from
much fewer images (even a single image) than required by classical approaches
as well as completion of unseen surfaces. We thoroughly evaluate our approach
on the ShapeNet dataset and demonstrate the benefits over classical approaches
as well as recent learning based methods.

83
Octree Generating Networks: Efficient Convolutional Architectures for High-resolution 3D Outputs
2017-03-28
We present a deep convolutional decoder architecture that can generate
volumetric 3D outputs in a compute- and memory-efficient manner by using an
octree representation. The network learns to predict both the structure of the
octree, and the occupancy values of individual cells. This makes it a
particularly valuable technique for generating 3D shapes. In contrast to
standard decoders acting on regular voxel grids, the architecture does not have
cubic complexity. This allows representing much higher resolution outputs with
a limited memory budget. We demonstrate this in several application domains,
including 3D convolutional autoencoders, generation of objects and whole scenes
from high-level representations, and shape from a single image.

86
Intrinsic3D: High-Quality 3D Reconstruction by Joint Appearance and Geometry Optimization with Spatially-Varying Lighting
2017-08-04
We introduce a novel method to obtain high-quality 3D reconstructions from
consumer RGB-D sensors. Our core idea is to simultaneously optimize for
geometry encoded in a signed distance field (SDF), textures from
automatically-selected keyframes, and their camera poses along with material
and scene lighting. To this end, we propose a joint surface reconstruction
approach that is based on Shape-from-Shading (SfS) techniques and utilizes the
estimation of spatially-varying spherical harmonics (SVSH) from subvolumes of
the reconstructed scene. Through extensive examples and evaluations, we
demonstrate that our method dramatically increases the level of detail in the
reconstructed scene geometry and contributes highly to consistent surface
texture recovery.

88
InfiniTAM v3: A Framework for Large-Scale 3D Reconstruction with Loop Closure
2017-08-02
Volumetric models have become a popular representation for 3D scenes in
recent years. One breakthrough leading to their popularity was KinectFusion,
which focuses on 3D reconstruction using RGB-D sensors. However, monocular SLAM
has since also been tackled with very similar approaches. Representing the
reconstruction volumetrically as a TSDF leads to most of the simplicity and
efficiency that can be achieved with GPU implementations of these systems.
However, this representation is memory-intensive and limits applicability to
small-scale reconstructions. Several avenues have been explored to overcome
this. With the aim of summarizing them and providing for a fast, flexible 3D
reconstruction pipeline, we propose a new, unifying framework called InfiniTAM.
The idea is that steps like camera tracking, scene representation and
integration of new data can easily be replaced and adapted to the user's needs.
  This report describes the technical implementation details of InfiniTAM v3,
the third version of our InfiniTAM system. We have added various new features,
as well as making numerous enhancements to the low-level code that
significantly improve our camera tracking performance. The new features that we
expect to be of most interest are (i) a robust camera tracking module; (ii) an
implementation of Glocker et al.'s keyframe-based random ferns camera
relocaliser; (iii) a novel approach to globally-consistent TSDF-based
reconstruction, based on dividing the scene into rigid submaps and optimising
the relative poses between them; and (iv) an implementation of Keller et al.'s
surfel-based reconstruction approach.

94
Superhuman Accuracy on the SNEMI3D Connectomics Challenge
2017-05-31
For the past decade, convolutional networks have been used for 3D
reconstruction of neurons from electron microscopic (EM) brain images. Recent
years have seen great improvements in accuracy, as evidenced by submissions to
the SNEMI3D benchmark challenge. Here we report the first submission to surpass
the estimate of human accuracy provided by the SNEMI3D leaderboard. A variant
of 3D U-Net is trained on a primary task of predicting affinities between
nearest neighbor voxels, and an auxiliary task of predicting long-range
affinities. The training data is augmented by simulated image defects. The
nearest neighbor affinities are used to create an oversegmentation, and then
supervoxels are greedily agglomerated based on mean affinity. The resulting
SNEMI3D score exceeds the estimate of human accuracy by a large margin. While
one should be cautious about extrapolating from the SNEMI3D benchmark to
real-world accuracy of large-scale neural circuit reconstruction, our result
inspires optimism that the goal of full automation may be realizable in the
future.

100
SMASH: Physics-guided Reconstruction of Collisions from Videos
2016-03-29
Collision sequences are commonly used in games and entertainment to add drama
and excitement. Authoring even two body collisions in the real world can be
difficult, as one has to get timing and the object trajectories to be correctly
synchronized. After tedious trial-and-error iterations, when objects can
actually be made to collide, then they are difficult to capture in 3D. In
contrast, synthetically generating plausible collisions is difficult as it
requires adjusting different collision parameters (e.g., object mass ratio,
coefficient of restitution, etc.) and appropriate initial parameters. We
present SMASH to directly read off appropriate collision parameters directly
from raw input video recordings. Technically we enable this by utilizing laws
of rigid body collision to regularize the problem of lifting 2D trajectories to
a physically valid 3D reconstruction of the collision. The reconstructed
sequences can then be modified and combined to easily author novel and
plausible collisions. We evaluate our system on a range of synthetic scenes and
demonstrate the effectiveness of our method by accurately reconstructing
several complex real world collision events.

101
3DMatch: Learning Local Geometric Descriptors from RGB-D Reconstructions
2016-03-27
Matching local geometric features on real-world depth images is a challenging
task due to the noisy, low-resolution, and incomplete nature of 3D scan data.
These difficulties limit the performance of current state-of-art methods, which
are typically based on histograms over geometric properties. In this paper, we
present 3DMatch, a data-driven model that learns a local volumetric patch
descriptor for establishing correspondences between partial 3D data. To amass
training data for our model, we propose a self-supervised feature learning
method that leverages the millions of correspondence labels found in existing
RGB-D reconstructions. Experiments show that our descriptor is not only able to
match local geometry in new scenes for reconstruction, but also generalize to
different tasks and spatial scales (e.g. instance-level object model alignment
for the Amazon Picking Challenge, and mesh surface correspondence). Results
show that 3DMatch consistently outperforms other state-of-the-art approaches by
a significant margin. Code, data, benchmarks, and pre-trained models are
available online at http://3dmatch.cs.princeton.edu

103
3D Object Reconstruction from Hand-Object Interactions
2017-04-03
Recent advances have enabled 3d object reconstruction approaches using a
single off-the-shelf RGB-D camera. Although these approaches are successful for
a wide range of object classes, they rely on stable and distinctive geometric
or texture features. Many objects like mechanical parts, toys, household or
decorative articles, however, are textureless and characterized by minimalistic
shapes that are simple and symmetric. Existing in-hand scanning systems and 3d
reconstruction techniques fail for such symmetric objects in the absence of
highly distinctive features. In this work, we show that extracting 3d hand
motion for in-hand scanning effectively facilitates the reconstruction of even
featureless and highly symmetric objects and we present an approach that fuses
the rich additional information of hands into a 3d reconstruction pipeline,
significantly contributing to the state-of-the-art of in-hand scanning.

111
SceneNet RGB-D: 5M Photorealistic Images of Synthetic Indoor Trajectories with Ground Truth
2016-12-15
We introduce SceneNet RGB-D, expanding the previous work of SceneNet to
enable large scale photorealistic rendering of indoor scene trajectories. It
provides pixel-perfect ground truth for scene understanding problems such as
semantic segmentation, instance segmentation, and object detection, and also
for geometric computer vision problems such as optical flow, depth estimation,
camera pose estimation, and 3D reconstruction. Random sampling permits
virtually unlimited scene configurations, and here we provide a set of 5M
rendered RGB-D images from over 15K trajectories in synthetic layouts with
random but physically simulated object poses. Each layout also has random
lighting, camera trajectories, and textures. The scale of this dataset is well
suited for pre-training data-driven computer vision techniques from scratch
with RGB-D inputs, which previously has been limited by relatively small
labelled datasets in NYUv2 and SUN RGB-D. It also provides a basis for
investigating 3D scene labelling tasks by providing perfect camera poses and
depth data as proxy for a SLAM system. We host the dataset at
http://robotvault.bitbucket.io/scenenet-rgbd.html

116
A Point Set Generation Network for 3D Object Reconstruction from a Single Image
2016-12-02
Generation of 3D data by deep neural network has been attracting increasing
attention in the research community. The majority of extant works resort to
regular representations such as volumetric grids or collection of images;
however, these representations obscure the natural invariance of 3D shapes
under geometric transformations and also suffer from a number of other issues.
In this paper we address the problem of 3D reconstruction from a single image,
generating a straight-forward form of output -- point cloud coordinates. Along
with this problem arises a unique and interesting issue, that the groundtruth
shape for an input image may be ambiguous. Driven by this unorthodox output
form and the inherent ambiguity in groundtruth, we design architecture, loss
function and learning paradigm that are novel and effective. Our final solution
is a conditional shape sampler, capable of predicting multiple plausible 3D
point clouds from an input image. In experiments not only can our system
outperform state-of-the-art methods on single image based 3d reconstruction
benchmarks; but it also shows a strong performance for 3d shape completion and
promising ability in making multiple plausible predictions.

120
Target-driven Visual Navigation in Indoor Scenes using Deep Reinforcement Learning
2016-09-16
Two less addressed issues of deep reinforcement learning are (1) lack of
generalization capability to new target goals, and (2) data inefficiency i.e.,
the model requires several (and often costly) episodes of trial and error to
converge, which makes it impractical to be applied to real-world scenarios. In
this paper, we address these two issues and apply our model to the task of
target-driven visual navigation. To address the first issue, we propose an
actor-critic model whose policy is a function of the goal as well as the
current state, which allows to better generalize. To address the second issue,
we propose AI2-THOR framework, which provides an environment with high-quality
3D scenes and physics engine. Our framework enables agents to take actions and
interact with objects. Hence, we can collect a huge number of training samples
efficiently.
  We show that our proposed method (1) converges faster than the
state-of-the-art deep reinforcement learning methods, (2) generalizes across
targets and across scenes, (3) generalizes to a real robot scenario with a
small amount of fine-tuning (although the model is trained in simulation), (4)
is end-to-end trainable and does not need feature engineering, feature matching
between frames or 3D reconstruction of the environment.
  The supplementary video can be accessed at the following link:
https://youtu.be/SmBxMDiOrvs.

133
Semantic 3D Reconstruction with Continuous Regularization and Ray Potentials Using a Visibility Consistency Constraint
2016-04-11
We propose an approach for dense semantic 3D reconstruction which uses a data term that is defined as potentials over viewing rays, combined with continuous surface area penalization. Our formulation is a convex relaxation which we augment with a crucial non-convex constraint that ensures exact handling of visibility. To tackle the non-convex minimization problem, we propose a majorize-minimize type strategy which converges to a critical point. We demonstrate the benefits of using the non-convex constraint experimentally. For the geometry-only case, we set a new state of the art on two datasets of the commonly used Middlebury multi-view stereo benchmark. Moreover, our general-purpose formulation directly reconstructs thin objects, which are usually treated with specialized algorithms. A qualitative evaluation on the dense semantic 3D reconstruction task shows that we improve significantly over previous methods.

138
3D-R2N2: A Unified Approach for Single and Multi-view 3D Object Reconstruction
2016-04-02
Inspired by the recent success of methods that employ shape priors to achieve
robust 3D reconstructions, we propose a novel recurrent neural network
architecture that we call the 3D Recurrent Reconstruction Neural Network
(3D-R2N2). The network learns a mapping from images of objects to their
underlying 3D shapes from a large collection of synthetic data. Our network
takes in one or more images of an object instance from arbitrary viewpoints and
outputs a reconstruction of the object in the form of a 3D occupancy grid.
Unlike most of the previous works, our network does not require any image
annotations or object class labels for training or testing. Our extensive
experimental analysis shows that our reconstruction framework i) outperforms
the state-of-the-art methods for single view reconstruction, and ii) enables
the 3D reconstruction of objects in situations when traditional SFM/SLAM
methods fail (because of lack of texture and/or wide baseline).

157
MOTChallenge 2015: Towards a Benchmark for Multi-Target Tracking
2015-04-08
In the recent past, the computer vision community has developed centralized
benchmarks for the performance evaluation of a variety of tasks, including
generic object and pedestrian detection, 3D reconstruction, optical flow,
single-object short-term tracking, and stereo estimation. Despite potential
pitfalls of such benchmarks, they have proved to be extremely helpful to
advance the state of the art in the respective area. Interestingly, there has
been rather limited work on the standardization of quantitative benchmarks for
multiple target tracking. One of the few exceptions is the well-known PETS
dataset, targeted primarily at surveillance applications. Despite being widely
used, it is often applied inconsistently, for example involving using different
subsets of the available data, different ways of training the models, or
differing evaluation scripts. This paper describes our work toward a novel
multiple object tracking benchmark aimed to address such issues. We discuss the
challenges of creating such a framework, collecting existing and new data,
gathering state-of-the-art methods to be tested on the datasets, and finally
creating a unified evaluation system. With MOTChallenge we aim to pave the way
toward a unified evaluation framework for a more meaningful quantification of
multi-target tracking.

172
ChESS - Quick and Robust Detection of Chess-board Features
2013-01-23
Localization of chess-board vertices is a common task in computer vision,
underpinning many applications, but relatively little work focusses on
designing a specific feature detector that is fast, accurate and robust. In
this paper the `Chess-board Extraction by Subtraction and Summation' (ChESS)
feature detector, designed to exclusively respond to chess-board vertices, is
presented. The method proposed is robust against noise, poor lighting and poor
contrast, requires no prior knowledge of the extent of the chess-board pattern,
is computationally very efficient, and provides a strength measure of detected
features. Such a detector has significant application both in the key field of
camera calibration, as well as in Structured Light 3D reconstruction. Evidence
is presented showing its robustness, accuracy, and efficiency in comparison to
other commonly used detectors both under simulation and in experimental 3D
reconstruction of flat plate and cylindrical objects

271
Photometric Bundle Adjustment for Dense Multi-View 3D Modeling
2014-06-01
Motivated by a Bayesian vision of the 3D multi-view reconstruction from images problem, we propose a dense 3D reconstruction technique that jointly refines the shape and the camera parameters of a scene by minimizing the photometric reprojection error between a generated model and the observed images, hence considering all pixels in the original images. The minimization is performed using a gradient descent scheme coherent with the shape representation (here a triangular mesh), where we derive evolution equations in order to optimize both the shape and the camera parameters. This can be used at a last refinement step in 3D reconstruction pipelines and helps improving the 3D reconstruction's quality by estimating the 3D shape and camera calibration more accurately. Examples are shown for multi-view stereo where the texture is also jointly optimized and improved, but could be used for any generative approaches dealing with multi-view reconstruction settings (i.e. depth map fusion, multi-view photometric stereo).

299
Structure-From-Motion Revisited
2016-06-01
Incremental Structure-from-Motion is a prevalent strategy for 3D reconstruction from unordered image collections. While incremental reconstruction systems have tremendously advanced in all regards, robustness, accuracy, completeness, and scalability remain the key problems towards building a truly general-purpose pipeline. We propose a new SfM technique that improves upon the state of the art to make a further step towards this ultimate goal. The full reconstruction pipeline is released to the public as an open-source implementation.

389
Single-View Reconstruction via Joint Analysis of Image and Shape Collections
2015-12-01
We present an approach to automatic 3D reconstruction of objects depicted in Web images. The approach reconstructs objects from single views. The key idea is to jointly analyze a collection of images of different objects along with a smaller collection of existing 3D models. The images are analyzed and reconstructed together. Joint analysis regularizes the formulated optimization problems, stabilizes correspondence estimation, and leads to reasonable reproduction of object appearance without traditional multi-view cues.

579
Thompson Sampling: An Asymptotically Optimal Finite Time Analysis
2012-05-18
The question of the optimality of Thompson Sampling for solving the stochastic multi-armed bandit problem had been open since 1933. In this paper we answer it positively for the case of Bernoulli rewards by providing the first finite-time analysis that matches the asymptotic rate given in the Lai and Robbins lower bound for the cumulative regret. The proof is accompanied by a numerical comparison with other optimal policies, experiments that have been lacking in the literature until now for the Bernoulli case.

1293
Reconstructing Three-Dimensional Models of Interacting Humans
2023-08-03
Understanding 3d human interactions is fundamental for fine-grained scene analysis and behavioural modeling. However, most of the existing models predict incorrect, lifeless 3d estimates, that miss the subtle human contact aspects--the essence of the event--and are of little use for detailed behavioral understanding. This paper addresses such issues with several contributions: (1) we introduce models for interaction signature estimation (ISP) encompassing contact detection, segmentation, and 3d contact signature prediction; (2) we show how such components can be leveraged to ensure contact consistency during 3d reconstruction; (3) we construct several large datasets for learning and evaluating 3d contact prediction and reconstruction methods; specifically, we introduce CHI3D, a lab-based accurate 3d motion capture dataset with 631 sequences containing $2,525$ contact events, $728,664$ ground truth 3d poses, as well as FlickrCI3D, a dataset of $11,216$ images, with $14,081$ processed pairs of people, and $81,233$ facet-level surface correspondences. Finally, (4) we propose methodology for recovering the ground-truth pose and shape of interacting people in a controlled setup and (5) annotate all 3d interaction motions in CHI3D with textual descriptions. Motion data in multiple formats (GHUM and SMPLX parameters, Human3.6m 3d joints) is made available for research purposes at \url{https://ci3d.imar.ro}, together with an evaluation server and a public benchmark.

1296
PlankAssembly: Robust 3D Reconstruction from Three Orthographic Views with Learnt Shape Programs
2023-08-10
In this paper, we develop a new method to automatically convert 2D line drawings from three orthographic views into 3D CAD models. Existing methods for this problem reconstruct 3D models by back-projecting the 2D observations into 3D space while maintaining explicit correspondence between the input and output. Such methods are sensitive to errors and noises in the input, thus often fail in practice where the input drawings created by human designers are imperfect. To overcome this difficulty, we leverage the attention mechanism in a Transformer-based sequence generation model to learn flexible mappings between the input and output. Further, we design shape programs which are suitable for generating the objects of interest to boost the reconstruction accuracy and facilitate CAD modeling applications. Experiments on a new benchmark dataset show that our method significantly outperforms existing ones when the inputs are noisy or incomplete.

